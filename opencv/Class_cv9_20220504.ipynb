{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b2a0032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c965c005",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = cv2.imread('./computer_vision/fig/fig/puppy.bmp')\n",
    "\n",
    "M = np.array([[1,0.2,0],\n",
    "             [0,1,0]], np.float32)\n",
    "\n",
    "dst = cv2.warpAffine(src, M, (0,0))\n",
    "# resize(src, dsize[, dst[, fx[, fy[, interpolation]]]]) -> dst\n",
    "dst_resize = cv2.resize(src, (0,0), None, fx =2, fy =1, \n",
    "                        interpolation = cv2.INTER_LINEAR)\n",
    "\n",
    "cv2.imshow('src', src)\n",
    "cv2.imshow('dst', dst)\n",
    "cv2.imshow('dst_resize', dst_resize)\n",
    "\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b05fe093",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 회전\n",
    "src = cv2.imread('./computer_vision/fig/fig/puppy.bmp')\n",
    "\n",
    "# rad = 30*np.pi/180\n",
    "# M = np.array([[np.cos(rad),np.sin(rad),0],\n",
    "#              [-np.sin(rad),np.cos(rad),0]], np.float32)\n",
    "\n",
    "h,w = src.shape[:2]\n",
    "\n",
    "cp = (w/2,h/2)\n",
    "M = cv2.getRotationMatrix2D(cp, 30, 1)\n",
    "\n",
    "dst = cv2.warpAffine(src, M, (0,0))\n",
    "\n",
    "cv2.imshow('src', src)\n",
    "cv2.imshow('dst', dst)\n",
    "\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0a1ed80",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = cv2.imread('./computer_vision/fig/green/checkerboard.png')\n",
    "\n",
    "h,w = src.shape[:2]\n",
    "\n",
    "src_point = np.array([[217,50],[691,47],[830,517],[67,526]],np.float32)\n",
    "\n",
    "dst_point = np.array([[0,0],[w-1,0],[w-1,h-1],[0,h-1]],np.float32)\n",
    "\n",
    "M = cv2.getPerspectiveTransform(src_point, dst_point)\n",
    "dst = cv2.warpPerspective(src, M, (w,h))\n",
    "\n",
    "cv2.imshow('src',src)\n",
    "cv2.imshow('dst',dst)\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ac9c558",
   "metadata": {},
   "outputs": [],
   "source": [
    "## blur\n",
    "src = cv2.imread('./computer_vision/fig/green/blue_eyes.png')\n",
    "\n",
    "kernel_3 = np.ones((3,3), np.float32)/9.\n",
    "kernel_5 = np.ones((5,5), np.float32)/25.\n",
    "\n",
    "\n",
    "dst3 = cv2.filter2D(src, -1, kernel_3)\n",
    "dst5 = cv2.filter2D(src, -1, kernel_5)\n",
    "dst_mean = cv2.blur(src, (7,7))\n",
    "\n",
    "dst_Gaussian = cv2.GaussianBlur(src, (0,0),5)\n",
    "\n",
    "dst_bilateral = cv2.bilateralFilter(src, -1, 10, 5)\n",
    "\n",
    "cv2.imshow('src',src)\n",
    "# cv2.imshow('dst3',dst3)\n",
    "# cv2.imshow('dst5',dst5)\n",
    "# cv2.imshow('dst_mean',dst_mean)\n",
    "cv2.imshow('dst_Gaussian',dst_Gaussian)\n",
    "cv2.imshow('dst_bilateral',dst_bilateral)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41002505",
   "metadata": {},
   "outputs": [],
   "source": [
    "## hist\n",
    "\n",
    "src = cv2.imread('./computer_vision/fig/green/spring_in_park.jpg',\n",
    "                cv2.IMREAD_REDUCED_COLOR_2)\n",
    "src_hsv = cv2.cvtColor(src, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "h,s,v = cv2.split(src_hsv)\n",
    "\n",
    "v_equal = cv2.equalizeHist(v)\n",
    "v_norm = cv2.normalize(v, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "src_equal = cv2.merge((h,s,v_equal))\n",
    "src_norm = cv2.merge((h,s,v_norm))\n",
    "\n",
    "src_equal = cv2.cvtColor(src_equal, cv2.COLOR_HSV2BGR)\n",
    "src_norm = cv2.cvtColor(src_norm, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "cv2.imshow('src',src)\n",
    "cv2.imshow('src_equal',src_equal)\n",
    "cv2.imshow('src_norm',src_norm)\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "add6049c",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.5) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:862: error: (-215:Assertion failed) trackbar in function 'cv::getTrackbarPos'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6184/879382661.py\u001b[0m in \u001b[0;36mcall_track\u001b[1;34m(pos)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcall_track\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mhmin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetTrackbarPos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'min'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'src'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mhmax\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetTrackbarPos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'max'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'src'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minRange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc_hsv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhmin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhmax\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.5.5) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:862: error: (-215:Assertion failed) trackbar in function 'cv::getTrackbarPos'\n"
     ]
    }
   ],
   "source": [
    "def call_track(pos):\n",
    "    hmin = cv2.getTrackbarPos('min', 'src')\n",
    "    hmax = cv2.getTrackbarPos('max', 'src')\n",
    "    \n",
    "    dst = cv2.inRange(src_hsv, (hmin,150,0), (hmax,255,255))\n",
    "    cv2.imshow('dst', dst)\n",
    "\n",
    "src = cv2.imread('./computer_vision/fig/green/palette_round.jpg')\n",
    "src_hsv = cv2.cvtColor(src, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "\n",
    "\n",
    "cv2.imshow('src',src)\n",
    "cv2.createTrackbar('min','src', 50, 179, call_track)\n",
    "cv2.createTrackbar('max','src', 80, 179, call_track)\n",
    "\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e426d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#backproj\n",
    "src = cv2.imread('./computer_vision/fig/fig/green.png',\n",
    "                cv2.IMREAD_REDUCED_COLOR_2)\n",
    "src_ycrcb = cv2.cvtColor(src, cv2.COLOR_BGR2YCrCb)\n",
    "\n",
    "\n",
    "x,y,w,h = cv2.selectROI(src)\n",
    "crop = src_ycrcb[y:y+h, x: x+w]\n",
    "\n",
    "channels = [1,2]\n",
    "ranges = [0,256,0,256]\n",
    "\n",
    "hist = cv2.calcHist([crop], channels, None, [256,256], ranges)\n",
    "\n",
    "\n",
    "backproj = cv2.calcBackProject([src_ycrcb],channels, hist, ranges,1)\n",
    "dst = cv2.copyTo(src, backproj)\n",
    "\n",
    "\n",
    "cv2.imshow('src',src)\n",
    "cv2.imshow('backproj',backproj)\n",
    "cv2.imshow('dst',dst)\n",
    "\n",
    "plt.imshow(hist)\n",
    "plt.show()\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41fa7018",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print('video open failed')\n",
    "    sys.exit()\n",
    "    \n",
    "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS)*0.7)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
    "\n",
    "\n",
    "out = cv2.VideoWriter('output.avi', fourcc, fps, (w,h))\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        print('frame read failed')\n",
    "        break\n",
    "    \n",
    "    cv2.imshow('frame', frame)\n",
    "    out.write(frame)\n",
    "    \n",
    "    if cv2.waitKey(30) == 27:\n",
    "        break\n",
    "        \n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35ac4137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_mouse(event, x,y, flags, param):\n",
    "    global oldx, oldy\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        oldx, oldy = x, y\n",
    "        \n",
    "    elif event == cv2.EVENT_MOUSEMOVE:\n",
    "        if flags & cv2.EVENT_FLAG_LBUTTON:\n",
    "            cv2.line(img, (oldx, oldy), (x,y), (0,0,255), 3)\n",
    "            cv2.imshow('image', img)\n",
    "            oldx, oldy = x,y\n",
    "            \n",
    "\n",
    "img = np.ones((500,600,3), np.uint8)*255\n",
    "\n",
    "cv2.imshow('image', img)\n",
    "cv2.setMouseCallback('image', call_mouse, img)\n",
    "\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e23e5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('./computer_vision/fig/fig/puppy.bmp')\n",
    "\n",
    "if img is None:\n",
    "    print('image read failed')\n",
    "    sys.exit()\n",
    "    \n",
    "cv2.namedWindow('image', cv2.WINDOW_AUTOSIZE)\n",
    "cv2.imshow('image', img)\n",
    "\n",
    "while True:\n",
    "    key = cv2.waitKey()\n",
    "    if key == 27 or key == ord('q'):\n",
    "        break\n",
    "        \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90545377",
   "metadata": {},
   "outputs": [],
   "source": [
    "------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2ab9bbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = './computer_vision/download/beagle/apple2.png'\n",
    "filename = './computer_vision/download/images/beagle.jpg'\n",
    "\n",
    "img = cv2.imread(filename)\n",
    "\n",
    "if img is None:\n",
    "    print('image read failed')\n",
    "    sys.exit()\n",
    "    \n",
    "model = './computer_vision/download/bvlc_googlenet.caffemodel'\n",
    "config = './computer_vision/download/deploy.prototxt'\n",
    "\n",
    "net = cv2.dnn.readNet(model, config)\n",
    "\n",
    "if net.empty():\n",
    "    print('Network load failed')\n",
    "    sys.exit()\n",
    "    \n",
    "classNames = []\n",
    "with open('./computer_vision/download/classification_classes_ILSVRC2012.txt','r') as f:\n",
    "    classNames = f.read().rstrip('\\n').split('\\n')\n",
    "\n",
    "blob = cv2.dnn.blobFromImage(img, 1, (244,244), (104,117,123),\n",
    "                            swapRB = False)    \n",
    "\n",
    "net.setInput(blob)\n",
    "prob = net.forward()\n",
    "# print(prob)\n",
    "out = prob.flatten()\n",
    "\n",
    "classId = np.argmax(out)\n",
    "confidence = out[classId]\n",
    "category = classNames[classId]\n",
    "text = f\"{category}'\\n'({confidence*100:4.2f} %)\"\n",
    "cv2.putText(img, text, (10, 30), cv2.FONT_HERSHEY_COMPLEX,\n",
    "           1, (0,0,255), 1, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "# print(classId)            # 948\n",
    "# print(classNames[classId]) # 'Granny Smith'\n",
    "# print(out[classId])         # 0.9995285\n",
    "\n",
    "\n",
    "# print(classNames)\n",
    "# print(classNames[0])\n",
    "\n",
    "cv2.imshow('img', img)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0f70c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = os.listdir('./computer_vision/download/images/')\n",
    "\n",
    "files = []\n",
    "\n",
    "for i in file_list:\n",
    "    file_path = './computer_vision/download/images/' + i\n",
    "    files.append(file_path)\n",
    "\n",
    "\n",
    "model = './computer_vision/download/googlenet/bvlc_googlenet.caffemodel'\n",
    "config = './computer_vision/download/googlenet/deploy.prototxt'\n",
    "\n",
    "net = cv2.dnn.readNet(model, config)\n",
    "\n",
    "if net.empty():\n",
    "    print('Network load failed')\n",
    "    sys.exit()    \n",
    "    \n",
    "\n",
    "classNames = []\n",
    "with open('./computer_vision/download/googlenet/classification_classes_ILSVRC2012.txt','r') as f:\n",
    "    classNames = f.read().rstrip('\\n').split('\\n')\n",
    "idx = 0\n",
    "while True:\n",
    "    file = files[idx]\n",
    "    img = cv2.imread(file)\n",
    "    \n",
    "    blob = cv2.dnn.blobFromImage(img, 1, (244,244), (104,117,123),\n",
    "                            swapRB = False)    \n",
    "\n",
    "    net.setInput(blob)\n",
    "    prob = net.forward()\n",
    "    # print(prob)\n",
    "    out = prob.flatten()\n",
    "\n",
    "    classId = np.argmax(out)\n",
    "    confidence = out[classId]\n",
    "    category = classNames[classId]\n",
    "    text = f\"{category}({confidence*100:4.2f} %)\"\n",
    "    cv2.putText(img, text, (10, 30), cv2.FONT_HERSHEY_COMPLEX,\n",
    "               1, (0,0,255), 1, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "    cv2.imshow('img', img)\n",
    "    \n",
    "    if cv2.waitKey(300) == 27:\n",
    "        break\n",
    "    \n",
    "    idx += 1\n",
    "    \n",
    "    if idx >= len(files):\n",
    "        idx = 0\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dfd6dd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 200, 7)\n"
     ]
    }
   ],
   "source": [
    "# 사진에서 얼굴 찾기\n",
    "img = cv2.imread('./computer_vision/download/images/sunglass.png')\n",
    "\n",
    "model = './computer_vision/download/facedetector/opencv_face_detector_uint8.pb'\n",
    "config = './computer_vision/download/facedetector/opencv_face_detector.pbtxt'\n",
    "\n",
    "# model = './computer_vision/download/facedetector/res10_300x300_ssd_iter_140000_fp16.caffemodel'\n",
    "# config = './computer_vision/download/facedetector/deploy.prototxt'\n",
    "\n",
    "\n",
    "face_detect_net = cv2.dnn.readNet(model, config)\n",
    "\n",
    "if face_detect_net.empty():\n",
    "    print('net load error')\n",
    "    sys.exit()\n",
    "\n",
    "blob = cv2.dnn.blobFromImage(img, 1, (300,300), (104,177,123),\n",
    "                            swapRB = False)\n",
    "\n",
    "face_detect_net.setInput(blob)\n",
    "out = face_detect_net.forward()\n",
    "print(out.shape) \n",
    "detect = out[0,0, :,:]    #(1,1, 200,7)에서 행 200 열 7 2행 :얼굴일확률, x1,y1,x2,y2 -> 확률로 나옴\n",
    "# print(detect.shape)       # 0~1행 버림 (200,7)\n",
    "\n",
    "h,w = img.shape[:2]\n",
    "\n",
    "for i in range(detect.shape[0]):   # 200\n",
    "    confidence = detect[i,2]\n",
    "    \n",
    "    if confidence > 0.5:\n",
    "        x1 = int(detect[i,3]*w)\n",
    "        y1 = int(detect[i,4]*h)\n",
    "        x2 = int(detect[i,5]*w)\n",
    "        y2 = int(detect[i,6]*h)\n",
    "        \n",
    "        cv2.rectangle(img, (x1,y1), (x2,y2), (0,0,255), 2)\n",
    "        text = 'Face : {}%'.format(round(confidence*100, 2))\n",
    "        cv2.putText(img, text, (x1, y1-2), cv2.FONT_HERSHEY_COMPLEX,\n",
    "                   0.8, (0,0,255), 1, cv2.LINE_AA)\n",
    "        \n",
    "\n",
    "cv2.imshow('image', img)\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "16c24753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영상에서 얼굴 찾기\n",
    "model = './computer_vision/download/facedetector/res10_300x300_ssd_iter_140000_fp16.caffemodel'\n",
    "config = './computer_vision/download/facedetector/deploy.prototxt'\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print('video open failed')\n",
    "    sys.exit()\n",
    "    \n",
    "net = cv2.dnn.readNet(model,config)\n",
    "\n",
    "if net.empty():\n",
    "    print('net load failed')\n",
    "    sys.exit()\n",
    "    \n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        print('frame read failed')\n",
    "        break\n",
    "    \n",
    "    blob = cv2.dnn.blobFromImage(frame, 1, (300,300), (104,177,123))\n",
    "    net.setInput(blob) # 네트워크로 들어가라\n",
    "    out = net.forward() #\n",
    "    \n",
    "    detect = out[0,0, :,:]\n",
    "    h,w = frame.shape[:2]\n",
    "    \n",
    "    for i in range(detect.shape[0]):\n",
    "        confidence = detect[i,2]\n",
    "        if confidence > 0.5:\n",
    "            x1 = int(detect[i,3]*w)\n",
    "            y1 = int(detect[i,4]*h)\n",
    "            x2 = int(detect[i,5]*w)\n",
    "            y2 = int(detect[i,6]*h)\n",
    "        \n",
    "        \n",
    "            cv2.rectangle(frame, (x1,y1), (x2,y2), (0,0,255), 2)\n",
    "            text = 'Face : {}%'.format(round(confidence*100, 2))\n",
    "            \n",
    "            cv2.putText(frame, text, (x1, y1-2), cv2.FONT_HERSHEY_COMPLEX,\n",
    "                        0.8, (0,0,255), 1, cv2.LINE_AA)\n",
    "        \n",
    "    cv2.imshow('frame', frame)\n",
    "    \n",
    "    \n",
    "    if cv2.waitKey(30) == 27:\n",
    "        break\n",
    "        \n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a5f95fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0de3c615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "[5 0 4 1 9 2 1 3 1 4]\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "# print(x_train.shape)\n",
    "# print(x_train.shape)\n",
    "# print(x_test.shape)\n",
    "\n",
    "x_train = x_train.reshape(60000, 28, 28, 1)/255.\n",
    "x_test = x_test.reshape(10000, 28, 28, 1)/255.\n",
    "\n",
    "# print(x_train.dtype)\n",
    "# print(x_test.dtype)\n",
    "\n",
    "# y_train = keras.utils.to_categorical(y_train)\n",
    "# y_train\n",
    "print(y_train.shape)\n",
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61c1b758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 24, 24, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 12, 12, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9216)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               1179776   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Conv2D(32, kernel_size = (3,3), input_shape = (28,28,1),\n",
    "                             activation = 'relu'))\n",
    "model.add(keras.layers.Conv2D(64, kernel_size = (3,3), activation = 'relu'))\n",
    "model.add(keras.layers.MaxPool2D(pool_size = 2))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(128, activation = 'relu'))\n",
    "model.add(keras.layers.Dropout(0.3))\n",
    "model.add(keras.layers.Dense(10, activation = 'softmax'))\n",
    "          \n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb70808d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'sparse_categorical_crossentropy',\n",
    "             optimizer = 'adam', \n",
    "              metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87c07c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "210/210 [==============================] - 52s 245ms/step - loss: 0.2689 - accuracy: 0.9183 - val_loss: 0.0819 - val_accuracy: 0.9758\n",
      "Epoch 2/10\n",
      "210/210 [==============================] - 48s 226ms/step - loss: 0.0738 - accuracy: 0.9780 - val_loss: 0.0578 - val_accuracy: 0.9832\n",
      "Epoch 3/10\n",
      "210/210 [==============================] - 50s 236ms/step - loss: 0.0511 - accuracy: 0.9842 - val_loss: 0.0514 - val_accuracy: 0.9848\n",
      "Epoch 4/10\n",
      "210/210 [==============================] - 48s 226ms/step - loss: 0.0385 - accuracy: 0.9883 - val_loss: 0.0497 - val_accuracy: 0.9856\n",
      "Epoch 5/10\n",
      "210/210 [==============================] - 48s 230ms/step - loss: 0.0292 - accuracy: 0.9908 - val_loss: 0.0481 - val_accuracy: 0.9878\n",
      "Epoch 6/10\n",
      "210/210 [==============================] - 48s 229ms/step - loss: 0.0235 - accuracy: 0.9920 - val_loss: 0.0490 - val_accuracy: 0.9868\n",
      "Epoch 7/10\n",
      "210/210 [==============================] - 50s 238ms/step - loss: 0.0208 - accuracy: 0.9929 - val_loss: 0.0522 - val_accuracy: 0.9869\n",
      "Epoch 8/10\n",
      "210/210 [==============================] - 53s 252ms/step - loss: 0.0173 - accuracy: 0.9941 - val_loss: 0.0424 - val_accuracy: 0.9893\n",
      "Epoch 9/10\n",
      "210/210 [==============================] - 50s 240ms/step - loss: 0.0140 - accuracy: 0.9953 - val_loss: 0.0479 - val_accuracy: 0.9874\n",
      "Epoch 10/10\n",
      "210/210 [==============================] - 52s 246ms/step - loss: 0.0124 - accuracy: 0.9959 - val_loss: 0.0475 - val_accuracy: 0.9887\n"
     ]
    }
   ],
   "source": [
    "modelpath = './computer_vision/mnist_mymodel/{epoch:002d}-{val_loss:.4f}.h5'\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath = modelpath,\n",
    "                                       save_best_only = True)\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience = 10)\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs = 10, batch_size = 200,\n",
    "                   verbose = 1, validation_split = 0.3,\n",
    "                   callbacks = [checkpoint, early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d8463dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 4s 13ms/step - loss: 0.0367 - accuracy: 0.9908\n",
      "[0.036735013127326965, 0.9908000230789185]\n"
     ]
    }
   ],
   "source": [
    "print(model.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f600078c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./mnist_onnx/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('./mnist_onnx/', include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48d229e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf2onnx\n",
      "  Downloading tf2onnx-1.9.3-py3-none-any.whl (435 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\mini\\anaconda3\\lib\\site-packages (from tf2onnx) (2.26.0)\n",
      "Requirement already satisfied: six in c:\\users\\mini\\anaconda3\\lib\\site-packages (from tf2onnx) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.14.1 in c:\\users\\mini\\anaconda3\\lib\\site-packages (from tf2onnx) (1.20.3)\n",
      "Collecting onnx>=1.4.1\n",
      "  Downloading onnx-1.11.0-cp39-cp39-win_amd64.whl (11.2 MB)\n",
      "Collecting flatbuffers~=1.12\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in c:\\users\\mini\\anaconda3\\lib\\site-packages (from onnx>=1.4.1->tf2onnx) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in c:\\users\\mini\\anaconda3\\lib\\site-packages (from onnx>=1.4.1->tf2onnx) (3.10.0.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\mini\\anaconda3\\lib\\site-packages (from requests->tf2onnx) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mini\\anaconda3\\lib\\site-packages (from requests->tf2onnx) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mini\\anaconda3\\lib\\site-packages (from requests->tf2onnx) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mini\\anaconda3\\lib\\site-packages (from requests->tf2onnx) (3.2)\n",
      "Installing collected packages: onnx, flatbuffers, tf2onnx\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 2.0\n",
      "    Uninstalling flatbuffers-2.0:\n",
      "      Successfully uninstalled flatbuffers-2.0\n",
      "Successfully installed flatbuffers-1.12 onnx-1.11.0 tf2onnx-1.9.3\n"
     ]
    }
   ],
   "source": [
    "! pip install -U tf2onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9803969d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 16:05:45.223858: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-05-04 16:05:45.223890: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "C:\\Users\\mini\\anaconda3\\lib\\runpy.py:127: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "2022-05-04 16:05:48.500197: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found\n",
      "2022-05-04 16:05:48.500216: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-04 16:05:48.503714: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: DESKTOP-U39MJLS\n",
      "2022-05-04 16:05:48.503778: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: DESKTOP-U39MJLS\n",
      "2022-05-04 16:05:48.504039: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-04 16:05:48,504 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
      "2022-05-04 16:05:48,812 - INFO - Signatures found in model: [serving_default].\n",
      "2022-05-04 16:05:48,812 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
      "2022-05-04 16:05:48,812 - INFO - Output names: ['dense_1']\n",
      "2022-05-04 16:05:48.814765: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2022-05-04 16:05:48.814918: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2022-05-04 16:05:48.825933: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1164] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: Graph size after: 48 nodes (36), 63 edges (51), time = 2.929ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\mini\\anaconda3\\lib\\site-packages\\tf2onnx\\tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-05-04 16:05:48,960 - WARNING - From C:\\Users\\mini\\anaconda3\\lib\\site-packages\\tf2onnx\\tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-05-04 16:05:48.966825: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2022-05-04 16:05:48.966957: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2022-05-04 16:05:49.021710: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1164] Optimization results for grappler item: graph_to_optimize\n",
      "  constant_folding: Graph size after: 32 nodes (-16), 47 edges (-16), time = 35.28ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.014ms.\n",
      "  constant_folding: Graph size after: 32 nodes (0), 47 edges (0), time = 9.542ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.003ms.\n",
      "\n",
      "2022-05-04 16:05:49,050 - INFO - Using tensorflow=2.8.0, onnx=1.11.0, tf2onnx=1.9.3/1190aa\n",
      "2022-05-04 16:05:49,050 - INFO - Using opset <onnx, 9>\n",
      "2022-05-04 16:05:49,335 - INFO - Computed 0 values for constant folding\n",
      "2022-05-04 16:05:49,519 - INFO - Optimizing ONNX model\n",
      "2022-05-04 16:05:49,726 - INFO - After optimization: Cast -1 (1->0), Const +1 (9->10), Identity -6 (6->0), Reshape +1 (1->2), Transpose -5 (6->1)\n",
      "2022-05-04 16:05:49,734 - INFO - \n",
      "2022-05-04 16:05:49,735 - INFO - Successfully converted TensorFlow model mnist_onnx to ONNX\n",
      "2022-05-04 16:05:49,735 - INFO - Model inputs: ['conv2d_2_input']\n",
      "2022-05-04 16:05:49,735 - INFO - Model outputs: ['dense_1']\n",
      "2022-05-04 16:05:49,735 - INFO - ONNX model is saved at model_mnist.onnx\n"
     ]
    }
   ],
   "source": [
    "!python -m tf2onnx.convert --saved-model mnist_onnx --output model_mnist.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77311a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (99.63%)\n",
      "2 (99.96%)\n",
      "3 (99.99%)\n",
      "4 (100.00%)\n",
      "3 (90.36%)\n",
      "5 (99.29%)\n",
      "6 (99.29%)\n",
      "7 (100.00%)\n",
      "8 (100.00%)\n",
      "9 (99.71%)\n",
      "0 (80.03%)\n",
      "0 (99.97%)\n"
     ]
    }
   ],
   "source": [
    "def on_mouse(event, x, y, flags, param):\n",
    "    global oldx, oldy\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        oldx, oldy = x,y\n",
    "    elif event == cv2.EVENT_MOUSEMOVE:\n",
    "        if flags & cv2.EVENT_FLAG_LBUTTON:\n",
    "            cv2.line(img, (oldx, oldy), (x,y), 255, 15, cv2.LINE_AA)\n",
    "            cv2.imshow('image', img)\n",
    "            oldx, oldy = x,y\n",
    "\n",
    "def norm_digit(img):\n",
    "    # 무게 중심 좌표 추출\n",
    "    m = cv2.moments(img)\n",
    "    cx = m['m10'] / m['m00']\n",
    "    cy = m['m01'] / m['m00']\n",
    "    h, w = img.shape[:2]\n",
    "    \n",
    "    # affine 행렬 생성\n",
    "    aff = np.array([[1, 0, w/2 - (cx + 0.5)], [0, 1, h/2 - (cy + 0.5)]], \n",
    "                   dtype=np.float32)\n",
    "    \n",
    "    # warpAffine을 이용해 기하학 변환\n",
    "    dst = cv2.warpAffine(img, aff, (0, 0))\n",
    "    return dst\n",
    "            \n",
    "                        \n",
    "net = cv2.dnn.readNet('./model_mnist.onnx')\n",
    "\n",
    "if net.empty():\n",
    "    print('Net load failed')\n",
    "    sys.exit()\n",
    "    \n",
    "img = np.zeros((400,400), np.uint8)\n",
    "cv2.imshow('image', img)\n",
    "cv2.setMouseCallback('image', on_mouse)\n",
    "\n",
    "while True:\n",
    "    key = cv2.waitKey()\n",
    "    \n",
    "    if key == 27:\n",
    "        break\n",
    "        \n",
    "    elif key == ord(' '):\n",
    "        blob= cv2.dnn.blobFromImage(norm_digit(img), 1/255., (28,28))\n",
    "        net.setInput(blob)\n",
    "        prob = net.forward()\n",
    "        \n",
    "        _, maxVal, _, maxLoc = cv2.minMaxLoc(prob)\n",
    "        digit = maxLoc[0]\n",
    "        \n",
    "        print(f'{digit} ({maxVal*100:4.2f}%)')\n",
    "        img.fill(0)\n",
    "        \n",
    "        cv2.imshow('image', img)\n",
    "        \n",
    "        \n",
    "        \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef91a032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.063014685639187e-12, 0.99993896484375, (0, 0), (4, 0))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.minMaxLoc(prob)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
